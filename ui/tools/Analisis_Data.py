"""
GoRide Sentiment Analysis - Data Analysis Module
===============================================

This module handles CSV file upload, text preprocessing, sentiment prediction,
and comprehensive analysis visualization for the GoRide sentiment analysis application.

Features:
- CSV file upload and validation
- Configurable text preprocessing
- Sentiment prediction with confidence scores
- Interactive visualizations (pie chart, gauge, bar charts)
- Word frequency and N-gram analysis
- Word cloud generation
- Text summarization
- Export functionality

Author: Mhd Adreansyah
Version: 2.0.0 (Rebuilt)
License: Copyright Protected (Tugas Akhir/Skripsi)
"""

# ==============================================================================
# IMPORTS
# ==============================================================================

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import time
import sys
import os
import traceback
import re
from collections import defaultdict
from typing import Dict, List, Optional, Tuple, Any
# (NLTK dihapus ‚Äì tidak lagi diperlukan setelah refactor)

# Authentication and utilities
from ui.auth import auth
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from utils import (
    load_sample_data, get_or_train_model, predict_sentiment,
    preprocess_text, get_word_frequencies, get_ngrams, create_wordcloud
)

# Caching ringan untuk menghindari model retrain & data reload tiap interaksi UI
@st.cache_resource(show_spinner=False)
def _cached_model_and_metrics():
    data = load_sample_data()
    preprocessing_options = DEFAULT_PREPROCESSING_OPTIONS.copy()
    pipeline, accuracy, precision, recall, f1, *_ = get_or_train_model(data, preprocessing_options)
    return data, preprocessing_options, pipeline, (accuracy, precision, recall, f1)

# ==============================================================================
# CONSTANTS AND CONFIGURATION
# ==============================================================================

# Default preprocessing options (compatible with utils.py)
DEFAULT_PREPROCESSING_OPTIONS = {
    'case_folding': True,
    'phrase_standardization': True,
    'cleansing': True,
    'normalize_slang': True,
    'remove_repeated': True,
    'tokenize': True,
    'remove_stopwords': True,
    'stemming': True,
    'rejoin': True
}

# UI Configuration
SENTIMENT_COLORS = {
    'POSITIF': 'green',
    'NEGATIF': 'red'
}

WORDCLOUD_COLOR_SCHEMES = [
    "viridis", "plasma", "inferno", "magma", 
    "cividis", "YlGnBu", "YlOrRd"
]

# Required columns for analysis
DISPLAY_COLUMNS = [
    'review_text', 'teks_preprocessing', 
    'predicted_sentiment', 'confidence'
]

# ==============================================================================
# HELPER FUNCTIONS FOR CALCULATIONS
# ==============================================================================

def calculate_sentiment_statistics(df: pd.DataFrame) -> Dict[str, Any]:
    """Hitung statistik inti sentimen (jumlah, persentase, rata-rata confidence)."""
    total_count = len(df)
    pos_count = len(df[df['predicted_sentiment'] == 'POSITIF'])
    neg_count = len(df[df['predicted_sentiment'] == 'NEGATIF'])
    
    pos_percentage = (pos_count / total_count * 100) if total_count > 0 else 0
    neg_percentage = (neg_count / total_count * 100) if total_count > 0 else 0
    avg_confidence = df['confidence'].mean() * 100 if not df['confidence'].empty else 0
    
    dominant_sentiment = "Positif" if pos_count >= neg_count else "Negatif"
    dominant_percentage = max(pos_percentage, neg_percentage)
    
    return {
        'total_count': total_count,
        'pos_count': pos_count,
        'neg_count': neg_count,
        'pos_percentage': pos_percentage,
        'neg_percentage': neg_percentage,
        'avg_confidence': avg_confidence,
        'dominant_sentiment': dominant_sentiment,
        'dominant_percentage': dominant_percentage
    }


def safe_progress_cleanup(progress_bar) -> None:
    """Bersihkan progress bar secara aman (abaikan error jika sudah dihapus)."""
    if progress_bar is not None:
        try:
            progress_bar.empty()
        except Exception:
            pass

def reset_analysis_state() -> None:
    """Reset state analisis (membersihkan hasil & pilihan sebelumnya)."""
    st.session_state.analysis_complete = False
    for key in ['csv_results', 'csv_preprocessed', 'preprocess_options', 'selected_text_column']:
        if key in st.session_state:
            del st.session_state[key]


def initialize_session_state() -> None:
    """Inisialisasi variabel state utama bila belum ada (refactor)."""
    defaults = {
        'analysis_complete': False,
        'csv_results': None,
        'preprocess_options': {},
        'selected_text_column': None
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v


def validate_dataframe(df: pd.DataFrame) -> Tuple[bool, str, Optional[str]]:
    """
    Validate uploaded DataFrame and identify text column.
    
    Args:
        df: Uploaded DataFrame
        
    Returns:
        Tuple of (is_valid, message, text_column_name)
    """
    if df.empty:
        return False, "‚ùå File CSV kosong!", None
    
    # Check if 'review_text' column exists
    if 'review_text' in df.columns:
        return True, "‚úÖ File CSV valid dengan kolom 'review_text'", 'review_text'
    
    # Look for potential text columns
    text_columns = []
    for col in df.columns:
        if df[col].dtype == 'object':  # Text columns are usually object type
            # Check if column contains meaningful text (not just numbers/short strings)
            sample_text = str(df[col].iloc[0]) if len(df) > 0 else ""
            if len(sample_text.split()) > 2:  # Has more than 2 words
                text_columns.append(col)
    
    if text_columns:
        return True, f"‚úÖ File CSV valid. Kolom teks ditemukan: {', '.join(text_columns)}", None
    else:
        return False, "‚ùå Tidak ditemukan kolom teks yang sesuai dalam file CSV!", None


def create_preprocessing_options_ui() -> Dict[str, bool]:
    """
    Create preprocessing options UI and return user selections.
    
    Returns:
        Dictionary of preprocessing options
    """
    st.write("### üõ†Ô∏è Opsi Preprocessing Teks")
    
    with st.expander("Pengaturan Preprocessing", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            case_folding = st.checkbox(
                "Konversi ke huruf kecil", 
                value=True, 
                key="analisis_data_case_folding",
                help="Mengubah semua huruf menjadi huruf kecil"
            )
            cleansing = st.checkbox(
                "Cleansing teks (URL, karakter khusus)", 
                value=True, 
                key="analisis_data_cleansing",
                help="Menghapus URL, emoji, dan karakter non-alfabetik"
            )
            normalize_slang = st.checkbox(
                "Normalisasi kata gaul/slang", 
                value=True, 
                key="analisis_data_normalize_slang",
                help="Mengubah kata gaul menjadi kata formal"
            )
            remove_repeated = st.checkbox(
                "Hapus karakter berulang", 
                value=True, 
                key="analisis_data_remove_repeated",
                help="Mengurangi karakter berulang (misal: 'bagusssss' ‚Üí 'baguss')"
            )
            tokenize = st.checkbox(
                "Tokenisasi teks", 
                value=True, 
                key="analisis_data_tokenize",
                help="Memecah teks menjadi token/kata individual"
            )
            
        with col2:
            remove_stopwords = st.checkbox(
                "Hapus stopwords", 
                value=True, 
                key="analisis_data_remove_stopwords",
                help="Menghapus kata-kata umum yang kurang bermakna"
            )
            stemming = st.checkbox(
                "Stemming (Sastrawi)", 
                value=True, 
                key="analisis_data_stemming",
                help="Mengubah kata ke bentuk dasarnya"
            )
            phrase_standardization = st.checkbox(
                "Standardisasi frasa", 
                value=True, 
                key="analisis_data_phrase_standardization",
                help="Menormalisasi frasa umum (misal: 'go-ride' ‚Üí 'goride')"
            )
            rejoin = st.checkbox(
                "Gabungkan kembali token", 
                value=True, 
                key="analisis_data_rejoin",
                help="Menggabungkan token kembali menjadi teks"
            )
    
    return {
        'case_folding': case_folding,
        'phrase_standardization': phrase_standardization,
        'cleansing': cleansing,
        'normalize_slang': normalize_slang,
        'remove_repeated': remove_repeated,
        'tokenize': tokenize,
        'remove_stopwords': remove_stopwords,
        'stemming': stemming,
        'rejoin': rejoin
    }


def process_uploaded_file(uploaded_file, preprocess_options: Dict[str, bool], 
                         pipeline) -> Tuple[bool, Optional[pd.DataFrame], str]:
    """
    Process uploaded CSV file and perform sentiment analysis.
    
    Args:
        uploaded_file: Streamlit uploaded file object
        preprocess_options: Preprocessing configuration
        pipeline: Trained sentiment analysis pipeline
        
    Returns:
        Tuple of (success, dataframe, message)
    """
    progress_bar = None
    
    try:
        # Progress bar setup
        progress_bar = st.progress(0, text="Memproses file CSV...")
        
        # Read CSV file with error handling
        try:
            df = pd.read_csv(uploaded_file, encoding='utf-8')
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(uploaded_file, encoding='latin-1')
                st.warning("‚ö†Ô∏è File dibaca dengan encoding latin-1. Pastikan karakter khusus tertampil dengan benar.")
            except Exception as e:
                safe_progress_cleanup(progress_bar)
                return False, None, f"‚ùå Gagal membaca file CSV: {str(e)}"
        
        progress_bar.progress(25, text="File berhasil diunggah...")
        
        # Validate file
        is_valid, message, text_col = validate_dataframe(df)
        if not is_valid:
            safe_progress_cleanup(progress_bar)
            return False, None, message
        
        # Handle column selection if needed
        if text_col is None:
            # Let user select text column
            text_columns = [col for col in df.columns if df[col].dtype == 'object']
            if not text_columns:
                safe_progress_cleanup(progress_bar)
                return False, None, "‚ùå Tidak ditemukan kolom teks dalam file CSV!"
            
            # Create column selector with better state management
            if 'selected_text_column' not in st.session_state:
                safe_progress_cleanup(progress_bar)
                
                st.write("**üéØ Pilih kolom yang berisi teks ulasan:**")
                
                with st.container():
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        selected_col = st.selectbox(
                            "Kolom teks:", 
                            text_columns, 
                            key="text_column_selector",
                            help="Pilih kolom yang berisi teks ulasan untuk dianalisis"
                        )
                    with col2:
                        if st.button("‚úÖ Konfirmasi", key="confirm_column", use_container_width=True):
                            st.session_state.selected_text_column = selected_col
                            st.rerun()
                
                # Show preview of selected column
                if selected_col and len(df) > 0:
                    st.write("**üëÄ Preview data dari kolom yang dipilih:**")
                    preview_data = df[selected_col].head(3).tolist()
                    for i, text in enumerate(preview_data):
                        st.text(f"{i+1}. {str(text)[:100]}...")
                
                return False, None, "Silakan pilih kolom teks dan konfirmasi untuk melanjutkan."
            else:
                df['review_text'] = df[st.session_state.selected_text_column]
        
        # Ensure review_text column exists
        if 'review_text' not in df.columns:
            safe_progress_cleanup(progress_bar)
            return False, None, "‚ùå Kolom review_text tidak ditemukan!"
        
        # Validate data quality
        df = df.dropna(subset=['review_text'])  # Remove empty reviews
        df = df[df['review_text'].astype(str).str.strip() != '']  # Remove empty strings
        
        if len(df) == 0:
            safe_progress_cleanup(progress_bar)
            return False, None, "‚ùå Tidak ada data teks yang valid untuk dianalisis!"
        
        progress_bar.progress(50, text="Melakukan preprocessing teks...")
        
        # Batch preprocess text for better performance
        try:
            df['teks_preprocessing'] = df['review_text'].astype(str).apply(
                lambda x: preprocess_text(x, preprocess_options)
            )
        except Exception as e:
            safe_progress_cleanup(progress_bar)
            return False, None, f"‚ùå Gagal melakukan preprocessing: {str(e)}"
        
        progress_bar.progress(75, text="Memprediksi sentimen...")
        
        # Predict sentiment with better error handling
        predicted_results = []
        successful_predictions = 0
        
        for i, text in enumerate(df['teks_preprocessing']):
            try:
                result = predict_sentiment(text, pipeline, preprocess_options)
                predicted_results.append(result)
                successful_predictions += 1
            except Exception as e:
                # Handle prediction errors gracefully
                predicted_results.append({
                    'sentiment': 'ERROR',
                    'confidence': 0.0,
                    'probabilities': {'POSITIF': 0.0, 'NEGATIF': 0.0}
                })
            
            # Update progress for large datasets
            if i % 100 == 0 and i > 0:
                current_progress = 75 + (i / len(df)) * 20
                progress_bar.progress(int(current_progress), text=f"Memprediksi sentimen... ({i}/{len(df)})")
        
        # Extract results
        df['predicted_sentiment'] = [result['sentiment'] for result in predicted_results]
        df['confidence'] = [result['confidence'] for result in predicted_results]
        
        # Filter out error predictions
        error_count = len(df[df['predicted_sentiment'] == 'ERROR'])
        if error_count > 0:
            st.warning(f"‚ö†Ô∏è {error_count} teks gagal diprediksi dan akan diabaikan.")
            df = df[df['predicted_sentiment'] != 'ERROR']
        
        if len(df) == 0:
            safe_progress_cleanup(progress_bar)
            return False, None, "‚ùå Semua prediksi gagal. Periksa kualitas data input."
        
        progress_bar.progress(100, text="Analisis selesai!")
        time.sleep(0.5)
        safe_progress_cleanup(progress_bar)
        
        success_rate = (successful_predictions / len(predicted_results)) * 100
        return True, df, f"‚úÖ Berhasil menganalisis {len(df)} ulasan! (Tingkat keberhasilan: {success_rate:.1f}%)"
        
    except Exception as e:
        safe_progress_cleanup(progress_bar)
        error_msg = f"‚ùå Terjadi kesalahan: {str(e)}"
        st.error(f"Debug info: {traceback.format_exc()}")
        return False, None, error_msg

# ==============================================================================
# VISUALIZATION FUNCTIONS
# ==============================================================================

def create_sentiment_metrics(df: pd.DataFrame, stats: Optional[Dict[str, Any]] = None) -> None:
    """Create clean and structured sentiment metrics display."""
    st.write("### üìä Hasil Analisis Sentimen")


def create_visualization_charts(df: pd.DataFrame, stats: Optional[Dict[str, Any]] = None) -> None:
    """Create clean and focused visualization charts for sentiment analysis."""
    
    # Use provided stats or calculate if not provided
    if stats is None:
        stats = calculate_sentiment_statistics(df)
    
    # Create two columns for clean layout
    col1, col2 = st.columns(2)
    
    with col1:
        # Pie chart for sentiment distribution
        sentiment_counts = df['predicted_sentiment'].value_counts().reset_index()
        sentiment_counts.columns = ['Sentiment', 'Count']
        
        fig_pie = px.pie(
            sentiment_counts, 
            values='Count', 
            names='Sentiment',
            color='Sentiment',
            color_discrete_map=SENTIMENT_COLORS,
            title="üìä Distribusi Sentimen",
            hover_data=['Count']
        )
        fig_pie.update_traces(
            textposition='inside', 
            textinfo='percent+label',
            textfont_size=14
        )
        fig_pie.update_layout(
            showlegend=True, 
            height=400,
            title_font_size=16,
            title_x=0.5
        )
        st.plotly_chart(fig_pie, use_container_width=True)
    
    with col2:
        # Gauge chart for positive sentiment percentage
        fig_gauge = go.Figure(go.Indicator(
            mode="gauge+number",
            value=stats['pos_percentage'],
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': "üìà Persentase Sentimen Positif", 'font': {'size': 16}},
            gauge={
                'axis': {'range': [0, 100]},
                'bar': {'color': "green" if stats['pos_percentage'] >= 50 else "red"},
                'steps': [
                    {'range': [0, 33], 'color': 'lightgray'},
                    {'range': [33, 66], 'color': 'gray'},
                    {'range': [66, 100], 'color': 'darkgray'}
                ],
                'threshold': {
                    'line': {'color': "black", 'width': 4},
                    'thickness': 0.75,
                    'value': stats['pos_percentage']
                }
            },
            number={'suffix': "%", 'valueformat': ".1f", 'font': {'size': 20}}
        ))
        fig_gauge.update_layout(height=400)
        st.plotly_chart(fig_gauge, use_container_width=True)

# ==============================================================================
# TAB CONTENT FUNCTIONS
# ==============================================================================

def render_results_table_tab(df: pd.DataFrame) -> None:
    """Render the results table tab with improved filtering and validation."""
    st.subheader("üìã Tabel Hasil Prediksi Sentimen")
    
    if df.empty:
        st.warning("‚ö†Ô∏è Tidak ada data untuk ditampilkan.")
        return
    
    # Filter options with better layout
    col1, col2 = st.columns([1, 3])
    
    with col1:
        filter_sentiment = st.selectbox(
            "Filter berdasarkan sentimen:",
            ["Semua", "POSITIF", "NEGATIF"],
            key="analisis_data_filter_sentiment",
            help="Filter hasil berdasarkan jenis sentimen"
        )
    
    with col2:
        confidence_threshold = st.slider(
            "Minimum confidence score:",
            min_value=0.0,
            max_value=1.0,
            value=0.0,
            step=0.05,
            key="analisis_data_confidence_threshold",
            help="Tampilkan hanya prediksi dengan confidence di atas threshold"
        )
    
    # Apply filters with validation
    filtered_df = df.copy()
    
    if filter_sentiment != "Semua":
        filtered_df = filtered_df[filtered_df['predicted_sentiment'] == filter_sentiment]
    
    filtered_df = filtered_df[filtered_df['confidence'] >= confidence_threshold]
    
    # Display filtered data with status
    if len(filtered_df) == 0:
        st.warning("‚ö†Ô∏è Tidak ada data yang memenuhi kriteria filter. Coba kurangi threshold confidence atau ubah filter sentimen.")
        
        # Show current filter summary
        st.info(f"""
        **Filter Saat Ini:**
        - Sentimen: {filter_sentiment}
        - Minimum Confidence: {confidence_threshold:.2f}
        
        **Saran:** Kurangi nilai minimum confidence atau pilih "Semua" untuk sentimen.
        """)
        return
    
    st.success(f"**Menampilkan {len(filtered_df):,} dari {len(df):,} ulasan** ({len(filtered_df)/len(df)*100:.1f}%)")
    
    # Select columns to display with error handling
    available_cols = [col for col in DISPLAY_COLUMNS if col in filtered_df.columns]
    if not available_cols:
        st.error("‚ùå Kolom yang diperlukan tidak tersedia dalam data.")
        return
    
    # Enhanced dataframe display
    st.dataframe(
        filtered_df[available_cols], 
        use_container_width=True,
        hide_index=True,
        column_config={
            "review_text": st.column_config.TextColumn(
                "Teks Asli",
                help="Teks ulasan asli sebelum preprocessing",
                max_chars=100
            ),
            "teks_preprocessing": st.column_config.TextColumn(
                "Teks Preprocessing",
                help="Teks setelah preprocessing",
                max_chars=100
            ),
            "predicted_sentiment": st.column_config.TextColumn(
                "Prediksi Sentimen",
                help="Hasil prediksi sentimen"
            ),
            "confidence": st.column_config.NumberColumn(
                "Confidence Score",
                help="Tingkat kepercayaan prediksi (0-1)",
                format="%.3f"
            )
        }
    )
    
    # Enhanced download functionality
    if not filtered_df.empty:
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            # Add timestamp to filename
            timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
            filename = f"hasil_prediksi_goride_{timestamp}.csv"
            
            csv = filtered_df.to_csv(index=False)
            st.download_button(
                label="üì• Download Hasil Prediksi (CSV)",
                data=csv,
                file_name=filename,
                mime="text/csv",
                use_container_width=True,
                help=f"Download {len(filtered_df)} data yang telah difilter"
            )


## Fungsi render_word_frequency_tab & render_ngram_analysis_tab dihapus (digantikan oleh render_word_analysis_tab)


def render_wordcloud_tab(df: pd.DataFrame) -> None:
    """Render dual word clouds (positive vs negative) with improved error handling."""
    st.subheader("‚òÅÔ∏è Word Cloud Per Sentimen")
    st.caption("Perbandingan kata dominan antara ulasan POSITIF dan NEGATIF.")

    if df is None or df.empty:
        st.warning("‚ö†Ô∏è Data tidak tersedia untuk membuat word cloud.")
        return

    if 'predicted_sentiment' not in df.columns:
        st.error("‚ùå Kolom 'predicted_sentiment' tidak ditemukan.")
        return

    # Pilih kolom teks
    text_col = 'teks_preprocessing' if 'teks_preprocessing' in df.columns else ('review_text' if 'review_text' in df.columns else None)
    if text_col is None:
        st.error("‚ùå Tidak ada kolom teks yang dapat digunakan.")
        return

    # Konfigurasi wordcloud dengan keys unik
    col_cfg1, col_cfg2, col_cfg3, col_cfg4 = st.columns(4)
    with col_cfg1:
        max_words = st.slider("Max Kata", 50, 300, 120, step=10, key="wordcloud_max_words")
    with col_cfg2:
        background_color = st.selectbox("Latar Belakang", ["white", "black", "lightgray"], index=0, key="wordcloud_bg")
    with col_cfg3:
        cmap_pos = st.selectbox("Skema Positif", WORDCLOUD_COLOR_SCHEMES, index=0, key="wordcloud_cmap_pos")
    with col_cfg4:
        cmap_neg = st.selectbox("Skema Negatif", WORDCLOUD_COLOR_SCHEMES, index=3, key="wordcloud_cmap_neg")

    # Siapkan data dengan error handling
    try:
        pos_series = df[df['predicted_sentiment'] == 'POSITIF'][text_col].dropna().astype(str)
        neg_series = df[df['predicted_sentiment'] == 'NEGATIF'][text_col].dropna().astype(str)
        
        pos_text = " ".join(pos_series.tolist()) if len(pos_series) > 0 else ""
        neg_text = " ".join(neg_series.tolist()) if len(neg_series) > 0 else ""
    except Exception as e:
        st.error(f"‚ùå Gagal memproses teks untuk wordcloud: {str(e)}")
        return

    def sufficient(text: str, min_unique: int = 5) -> bool:
        """Check if text has enough unique words for wordcloud"""
        if not text or not text.strip():
            return False
        unique_words = {t.strip() for t in text.split() if t.strip() and len(t.strip()) > 1}
        return len(unique_words) >= min_unique

    col_pos, col_neg = st.columns(2)
    any_generated = False
    
    # Wordcloud Positif
    with col_pos:
        st.markdown("### üòä Positif")
        if sufficient(pos_text):
            try:
                with st.spinner("Membuat wordcloud positif..."):
                    wc_pos = create_wordcloud(pos_text, max_words=max_words, background_color=background_color, colormap=cmap_pos)
                    if wc_pos is not None:
                        st.image(wc_pos.to_array(), use_column_width=True)
                        any_generated = True
                        if hasattr(wc_pos, 'words_') and wc_pos.words_:
                            top_pos = max(wc_pos.words_.items(), key=lambda x: x[1])[0]
                            st.caption(f"Kata tersering: {top_pos}")
                    else:
                        st.info("Tidak dapat membentuk word cloud positif.")
            except Exception as e:
                st.error(f"‚ùå Error membuat wordcloud positif: {str(e)}")
                st.info("Coba ubah pengaturan atau periksa kualitas data.")
        else:
            st.info(f"Data positif belum cukup (kata unik < 5). Data tersedia: {len(pos_series)} ulasan")
    
    # Wordcloud Negatif
    with col_neg:
        st.markdown("### üòû Negatif")
        if sufficient(neg_text):
            try:
                with st.spinner("Membuat wordcloud negatif..."):
                    wc_neg = create_wordcloud(neg_text, max_words=max_words, background_color=background_color, colormap=cmap_neg)
                    if wc_neg is not None:
                        st.image(wc_neg.to_array(), use_column_width=True)
                        any_generated = True
                        if hasattr(wc_neg, 'words_') and wc_neg.words_:
                            top_neg = max(wc_neg.words_.items(), key=lambda x: x[1])[0]
                            st.caption(f"Kata tersering: {top_neg}")
                    else:
                        st.info("Tidak dapat membentuk word cloud negatif.")
            except Exception as e:
                st.error(f"‚ùå Error membuat wordcloud negatif: {str(e)}")
                st.info("Coba ubah pengaturan atau periksa kualitas data.")
        else:
            st.info(f"Data negatif belum cukup (kata unik < 5). Data tersedia: {len(neg_series)} ulasan")

    # Statistik
    st.markdown("---")
    col_m1, col_m2, col_m3 = st.columns(3)
    with col_m1:
        st.metric("Ulasan Positif", f"{len(pos_series):,}")
    with col_m2:
        st.metric("Ulasan Negatif", f"{len(neg_series):,}")
    with col_m3:
        try:
            vocab_union = len({*pos_text.split(), *neg_text.split()}) if (pos_text or neg_text) else 0
            st.metric("Total Vokab Unik Gabungan", f"{vocab_union:,}")
        except Exception:
            st.metric("Total Vokab Unik Gabungan", "N/A")

    if not any_generated:
        st.warning("‚ö†Ô∏è Word cloud belum dapat ditampilkan. Pastikan data memiliki cukup variasi kata atau ubah preprocessing.")
    st.caption("üí° Word cloud ditampilkan jika tiap kategori memiliki ‚â•5 kata unik. Pastikan preprocessing menghasilkan teks yang bervariasi.")


## Fungsi ringkasan teks manual dihapus ‚Äì digantikan oleh tab Insights & analisis aspek.

# ==============================================================================
# MAIN ANALYSIS TABS
# ==============================================================================

def render_analysis_tabs(df: pd.DataFrame, preprocessed_text: str) -> None:
    """Render semua tab analisis menggunakan st.tabs (persist otomatis oleh Streamlit)."""
    tabs = st.tabs([
        "üìã Tabel Hasil",
        "üìù Analisis Kata",
        "‚òÅÔ∏è Word Cloud",
        "üí° Insights & Rekomendasi"
    ])

    with tabs[0]:
        render_results_table_tab(df)
    with tabs[1]:
        render_word_analysis_tab(df, preprocessed_text)
    with tabs[2]:
        render_wordcloud_tab(df)
    with tabs[3]:
        render_insights_recommendations_tab(df)


def render_insights_recommendations_tab(df: pd.DataFrame) -> None:
    """Render tab Insights & Rekomendasi (adaptasi dari dashboard ringkasan).

    Mengganti Ringkasan Teks dengan analisis aspek, status keseluruhan,
    temuan utama, rekomendasi dinamis, dan narasi ringkas.
    Kolom sentimen menggunakan 'predicted_sentiment' (bukan 'sentiment').
    """
    st.subheader("üí° Insights & Rekomendasi")

    if df is None or df.empty:
        st.warning("‚ö†Ô∏è Data tidak tersedia untuk menghasilkan insights.")
        return

    if 'predicted_sentiment' not in df.columns:
        st.error("‚ùå Kolom 'predicted_sentiment' tidak ada. Jalankan analisis terlebih dahulu.")
        return

    # =====================
    # 1. METRIK DASAR
    # =====================
    total_reviews = len(df)
    pos_count = int((df['predicted_sentiment'] == 'POSITIF').sum())
    neg_count = int((df['predicted_sentiment'] == 'NEGATIF').sum())
    pos_pct = (pos_count / total_reviews * 100) if total_reviews else 0
    neg_pct = (neg_count / total_reviews * 100) if total_reviews else 0

    # =====================
    # 2. TREN 7 HARI (jika ada kolom date)
    # =====================
    trend_change: float = 0.0
    if 'date' in df.columns:
        try:
            df_ts = df.copy()
            df_ts['date'] = pd.to_datetime(df_ts['date'], errors='coerce')
            recent_cut = pd.Timestamp.now() - pd.Timedelta(days=7)
            prev_cut = recent_cut - pd.Timedelta(days=7)
            recent = df_ts[df_ts['date'] >= recent_cut]
            prev = df_ts[(df_ts['date'] >= prev_cut) & (df_ts['date'] < recent_cut)]
            if len(prev) > 0 and len(recent) > 0:
                recent_pos = (recent['predicted_sentiment'] == 'POSITIF').mean() * 100
                prev_pos = (prev['predicted_sentiment'] == 'POSITIF').mean() * 100
                trend_change = recent_pos - prev_pos
        except Exception:
            trend_change = 0.0

    # =====================
    # 3. STATUS OVERALL
    # =====================
    if pos_pct >= 75 and trend_change >= 3:
        status_label = "Sangat Baik"
        status_desc = "Kepuasan tinggi dan meningkat"
        status_color = "#16a34a"
    elif pos_pct < 55 or trend_change <= -3:
        status_label = "Perlu Perhatian"
        status_desc = "Perlu tindakan perbaikan segera"
        status_color = "#dc2626"
    else:
        status_label = "Stabil"
        status_desc = "Sentimen relatif stabil"
        status_color = "#2563eb"

    # =====================
    # 4. ANALISIS ASPEK
    # =====================
    @st.cache_data(show_spinner=False)
    def build_aspect_lexicon():
        return {
            'driver': {'driver','pengemudi','ojek','kurir'},
            'aplikasi': {'aplikasi','app','apps','fitur','versi','update'},
            'harga': {'harga','tarif','biaya','ongkos','mahal','murah'},
            'waktu': {'waktu','lama','cepat','delay','menunggu','nunggu','tunggu'},
            'pembayaran': {'bayar','pembayaran','gopay','cash','tunai','saldo'},
            'promosi': {'promo','promosi','diskon','voucher','potongan'},
            'keamanan': {'aman','keamanan','bahaya','kecelakaan','nabrak'},
            'kenyamanan': {'nyaman','kenyamanan','bersih','panas','bau','helm'},
            'layanan': {'layanan','service','respon','customer','cs'},
            'performa': {'error','bug','lambat','lemot','lag','crash','force','close'}
        }

    STOP_TOKENS = {"nya","yang","itu","di","ke","ku","lah","pun","deh","dong","nih","ya","sih"}

    def normalize_token(tok: str) -> str:
        tok = tok.lower()
        tok = re.sub(r'(nya|lah|kah|pun)$','', tok)
        return tok

    def extract_aspect_stats(df_local: pd.DataFrame) -> dict:
        if 'teks_preprocessing' not in df_local.columns:
            return {}
        lex = build_aspect_lexicon()
        inverse = {w: a for a, ws in lex.items() for w in ws}
        def _new_stat():
            return {"pos": 0, "neg": 0, "total": 0, "pos_ids": set(), "neg_ids": set()}
        stats = defaultdict(_new_stat)
        for idx, row in df_local.iterrows():
            sent = row.get('predicted_sentiment','')
            tokens = str(row.get('teks_preprocessing','')).split()
            aspects_found = set()
            for t in tokens:
                t_norm = normalize_token(t)
                if t_norm in STOP_TOKENS or len(t_norm) < 2:
                    continue
                if t_norm in inverse:
                    aspects_found.add(inverse[t_norm])
            for a in aspects_found:
                stats[a]['total'] += 1
                if sent == 'POSITIF':
                    stats[a]['pos'] += 1
                    stats[a]['pos_ids'].add(idx)
                elif sent == 'NEGATIF':
                    stats[a]['neg'] += 1
                    stats[a]['neg_ids'].add(idx)
        return stats

    def score_aspects(stats: dict, total_neg: int):
        scored = []
        for a, s in stats.items():
            total_a = s['total']
            if total_a < max(5, int(0.01 * total_reviews)):
                continue
            pos_a, neg_a = s['pos'], s['neg']
            sentiment_score = (pos_a - neg_a) / max(1, total_a)
            impact = neg_a / max(1, total_neg)
            opportunity = impact * (1 - (sentiment_score + 1)/2)
            scored.append({
                'aspect': a,
                'total': total_a,
                'pos': pos_a,
                'neg': neg_a,
                'sentiment_score': sentiment_score,
                'impact': impact,
                'opportunity': opportunity
            })
        return scored

    def generate_dynamic_recommendations(improvement_areas: list) -> list:
        templates = {
            'driver': 'Perkuat pelatihan & quality control driver untuk konsistensi layanan.',
            'aplikasi': 'Optimalkan stabilitas & UX aplikasi; prioritaskan perbaikan bug paling sering.',
            'harga': 'Evaluasi struktur tarif & transparansi biaya perjalanan.',
            'waktu': 'Kurangi waktu tunggu dengan optimasi algoritma penugasan & estimasi.',
            'pembayaran': 'Perbaiki reliabilitas metode pembayaran & jelaskan kegagalan transaksi.',
            'promosi': 'Sesuaikan komunikasi promo agar relevan & mudah ditebus pengguna.',
            'keamanan': 'Tingkatkan edukasi & standar keselamatan perjalanan.',
            'kenyamanan': 'Pastikan standar kebersihan & kelayakan perlengkapan keselamatan.',
            'layanan': 'Tingkatkan kecepatan/respons tim dukungan & proaktif tangani keluhan.',
            'performa': 'Optimalkan performa aplikasi (load time, crash rate).'
        }
        recs = []
        for item in improvement_areas:
            a = item['aspect']
            base = templates.get(a, f'Perbaiki kualitas aspek {a}.')
            recs.append(f"{a.capitalize()}: {base}")
        if not recs:
            recs.append('Pertahankan kualitas layanan & eksplor program loyalitas untuk meningkatkan retensi.')
        return recs

    stats_aspect = extract_aspect_stats(df)
    total_neg = int((df['predicted_sentiment'] == 'NEGATIF').sum())
    scored = score_aspects(stats_aspect, total_neg)
    positive_highlights = sorted(
        [x for x in scored if x['sentiment_score'] > 0.25],
        key=lambda d: (d['sentiment_score'], d['total']), reverse=True)[:5]
    improvement_areas = sorted(
        [x for x in scored if x['sentiment_score'] < -0.15],
        key=lambda d: (d['opportunity'], d['neg']), reverse=True)[:5]

    # =====================
    # 5. KARTU STATUS
    # =====================
    st.markdown("#### üìä Analisis Sentimen Saat Ini")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown(f"""
        <div style='border-left:4px solid {status_color};padding:0.75rem 0.9rem;background:#11182712;border-radius:6px;'>
            <h5 style='margin:0;color:{status_color};'>Status: {status_label}</h5>
            <div style='font-size:0.85rem;color:#555;'>{status_desc}</div>
            <div style='margin-top:0.5rem;font-weight:600;'>{pos_pct:.1f}% Positif</div>
        </div>
        """, unsafe_allow_html=True)
    with col2:
        st.markdown(f"""
        <div style='border-left:4px solid #0d9488;padding:0.75rem 0.9rem;background:#11182712;border-radius:6px;'>
            <h5 style='margin:0;color:#0d9488;'>Volume Ulasan</h5>
            <div style='font-size:0.85rem;color:#555;'>Total ulasan dianalisis</div>
            <div style='margin-top:0.5rem;font-weight:600;'>{total_reviews:,} Ulasan</div>
        </div>
        """, unsafe_allow_html=True)
    with col3:
        trend_color = '#16a34a' if trend_change > 0 else ('#dc2626' if trend_change < 0 else '#2563eb')
        st.markdown(f"""
        <div style='border-left:4px solid {trend_color};padding:0.75rem 0.9rem;background:#11182712;border-radius:6px;'>
            <h5 style='margin:0;color:{trend_color};'>Tren 7 Hari</h5>
            <div style='font-size:0.85rem;color:#555;'>Perubahan persentase positif</div>
            <div style='margin-top:0.5rem;font-weight:600;'>{trend_change:+.1f} p.p</div>
        </div>
        """, unsafe_allow_html=True)

    # =====================
    # 6. TEMUAN UTAMA
    # =====================
    st.markdown("---")
    st.markdown("#### üîç Temuan Utama Berbasis Aspek")
    if not scored:
        st.info("Belum ada aspek terdeteksi atau preprocessing belum menghasilkan teks.")
        return
    col_pos, col_neg = st.columns(2)
    with col_pos:
        st.markdown("**‚úÖ Aspek Positif yang Menonjol**")
        if positive_highlights:
            for item in positive_highlights:
                st.markdown(f"‚Ä¢ **{item['aspect'].capitalize()}** ‚Äì {item['total']} ulasan (skor {item['sentiment_score']:.2f})")
        else:
            st.markdown("_Tidak ada aspek dengan keunggulan signifikan saat ini_")
    with col_neg:
        st.markdown("**‚ö†Ô∏è Aspek yang Perlu Diperbaiki**")
        if improvement_areas:
            for item in improvement_areas:
                st.markdown(f"‚Ä¢ **{item['aspect'].capitalize()}** ‚Äì {item['neg']} keluhan (kontribusi {item['impact']*100:.1f}% negatif)")
        else:
            st.markdown("_Tidak ada aspek negatif dominan ‚Äì fokus pada penguatan_")

    # =====================
    # 7. REKOMENDASI DINAMIS
    # =====================
    st.markdown("---")
    st.markdown("#### üéØ Rekomendasi Tindakan Prioritas")
    recs = generate_dynamic_recommendations(improvement_areas)
    for r in recs:
        st.markdown(f"- {r}")

    # =====================
    # 8. NARASI RINGKAS
    # =====================
    st.markdown("---")
    narrative = []
    narrative.append(f"Dari {total_reviews} ulasan, {pos_pct:.1f}% bernada positif dan {neg_pct:.1f}% negatif.")
    if trend_change != 0:
        narrative.append(f"Sentimen {'meningkat' if trend_change>0 else 'menurun'} {abs(trend_change):.1f} p.p dalam 7 hari terakhir.")
    if improvement_areas:
        top_imp = improvement_areas[0]
        narrative.append(f"Aspek prioritas: {top_imp['aspect']} (kontribusi {top_imp['impact']*100:.1f}% keluhan).")
    st.markdown("**Ringkasan:** " + " ".join(narrative))
    st.caption("Aspek dihitung sekali per ulasan; token tidak bermakna seperti 'nya' diabaikan.")


def render_word_analysis_tab(df: pd.DataFrame, preprocessed_text: str) -> None:
    """Unified word analysis (frequency + N-gram) mirroring best pattern from dashboard summary (tanpa wordcloud)."""

    st.subheader("üìù Analisis Kata & Frasa")
    st.markdown(
        """
        *Temukan kata paling sering muncul dan kombinasi kata (N-Gram) untuk memahami pola umum dalam ulasan.*
        - Mode default menampilkan ringkasan frekuensi kata.
        - Aktifkan segmentasi per sentimen untuk membandingkan kata dominan antara ulasan positif dan negatif.
        - Bagian N-Gram tersedia di bawah dengan opsi pengaturan tambahan.
        """
    )

    if not preprocessed_text or not preprocessed_text.strip():
        st.warning("‚ö†Ô∏è Tidak ada teks yang tersedia untuk dianalisis.")
        return

    # =============================
    # 1. PENGATURAN UTAMA (Ringkas)
    # =============================
    col_main1, col_main2, col_main3 = st.columns([1, 1, 1])
    with col_main1:
        top_n_words = st.slider(
            "Top Kata:", 5, 50, 10, step=5,
            key="word_analysis_top_n",
            help="Jumlah kata teratas yang akan divisualisasikan"
        )
    with col_main2:
        segment_mode = st.selectbox(
            "Mode Frekuensi:", ["Gabungan", "Per Sentimen"], index=0,
            key="word_analysis_segment_mode",
            help="Pilih 'Per Sentimen' untuk melihat perbandingan POSITIF vs NEGATIF"
        )
    with col_main3:
        orientation = st.radio(
            "Orientasi Bar:", ["Horizontal", "Vertikal"], index=0,
            key="word_analysis_orientation",
            help="Ubah orientasi tampilan chart kata"
        )

    with st.expander("‚öôÔ∏è Pengaturan Tambahan (Opsional)"):
        st.markdown("Gunakan pengaturan lanjutan di bawah ini jika diperlukan penyesuaian presentasi.")
        col_adv1, col_adv2 = st.columns(2)
        with col_adv1:
            min_token_len = st.number_input(
                "Minimal panjang token ikut dihitung", min_value=1, max_value=5, value=2, step=1,
                key="word_analysis_min_token_len",
                help="Token lebih pendek dari nilai ini akan diabaikan dalam perhitungan frekuensi"
            )
        with col_adv2:
            show_tables_expanded = st.checkbox(
                "Buka tabel frekuensi secara default", value=False,
                key="word_analysis_show_tables_expanded",
                help="Jika aktif, tabel rincian langsung terbuka tanpa perlu diklik"
            )

    # Helper to build frequency from tokenized texts
    def build_freq(series: pd.Series) -> Dict[str, int]:
        freq: Dict[str, int] = {}
        for text in series.dropna().astype(str):
            for tok in text.split():
                if not tok or len(tok) < min_token_len:
                    continue
                freq[tok] = freq.get(tok, 0) + 1
        return dict(sorted(freq.items(), key=lambda x: x[1], reverse=True))

    # Ensure we have per-row preprocessed text; fallback ke preprocessed_text global
    if 'teks_preprocessing' in df.columns:
        overall_freq = build_freq(df['teks_preprocessing'])
        pos_freq = build_freq(df[df['predicted_sentiment'] == 'POSITIF']['teks_preprocessing']) if 'predicted_sentiment' in df.columns else {}
        neg_freq = build_freq(df[df['predicted_sentiment'] == 'NEGATIF']['teks_preprocessing']) if 'predicted_sentiment' in df.columns else {}
    else:
        # Fallback: gunakan gabungan teks global
        overall_freq = get_word_frequencies(preprocessed_text, top_n=top_n_words)
        pos_freq, neg_freq = {}, {}

    # =============================
    # 2. VISUALISASI FREKUENSI KATA
    # =============================
    st.markdown("### üìä Frekuensi Kata")

    if segment_mode == "Gabungan":
        if not overall_freq:
            st.info("üìù Tidak ada kata yang cukup untuk dianalisis.")
        else:
            top_items = list(overall_freq.items())[:top_n_words]
            freq_df = pd.DataFrame(top_items, columns=["Kata", "Frekuensi"])
            if orientation == "Horizontal":
                freq_df_plot = freq_df.sort_values('Frekuensi', ascending=True)
                fig = px.bar(
                    freq_df_plot,
                    x='Frekuensi', y='Kata', orientation='h',
                    title=f"Top {len(freq_df_plot)} Kata Teratas (Gabungan)",
                    color='Frekuensi', color_continuous_scale='Viridis'
                )
            else:
                fig = px.bar(
                    freq_df.head(top_n_words),
                    x='Kata', y='Frekuensi',
                    title=f"Top {len(freq_df)} Kata Teratas (Gabungan)",
                    color='Frekuensi', color_continuous_scale='Viridis'
                )
                fig.update_xaxes(tickangle=45)
            fig.update_layout(height=500, showlegend=False)
            st.plotly_chart(fig, use_container_width=True)
            coverage = (freq_df['Frekuensi'].sum() / max(sum(overall_freq.values()), 1)) * 100
            colm1, colm2, colm3 = st.columns(3)
            with colm1:
                st.metric("Kata Ditampilkan", len(freq_df))
            with colm2:
                st.metric("Total Kata Unik", f"{len(overall_freq):,}")
            with colm3:
                st.metric("Cakupan Top Kata", f"{coverage:.1f}%")
            with st.expander("üìã Tabel Frekuensi (Gabungan)", expanded=show_tables_expanded):
                st.dataframe(freq_df, use_container_width=True, hide_index=True)
    else:
        colA, colB = st.columns(2)
        with colA:
            st.markdown("**üòä Positif**")
            if not pos_freq:
                st.info("Tidak ada data positif atau kolom sentimen tidak tersedia.")
            else:
                pos_items = list(pos_freq.items())[:top_n_words]
                pos_df = pd.DataFrame(pos_items, columns=["Kata", "Frekuensi"])
                pos_df_plot = pos_df.sort_values('Frekuensi', ascending=True)
                fig_pos = px.bar(
                    pos_df_plot,
                    x='Frekuensi', y='Kata', orientation='h',
                    title=f"Top {len(pos_df_plot)} Kata Positif",
                    color='Frekuensi', color_continuous_scale='Greens'
                )
                fig_pos.update_layout(height=500, showlegend=False)
                st.plotly_chart(fig_pos, use_container_width=True)
                with st.expander("üìã Tabel Kata Positif", expanded=show_tables_expanded):
                    st.dataframe(pos_df, use_container_width=True, hide_index=True)
        with colB:
            st.markdown("**üòû Negatif**")
            if not neg_freq:
                st.info("Tidak ada data negatif atau kolom sentimen tidak tersedia.")
            else:
                neg_items = list(neg_freq.items())[:top_n_words]
                neg_df = pd.DataFrame(neg_items, columns=["Kata", "Frekuensi"])
                neg_df_plot = neg_df.sort_values('Frekuensi', ascending=True)
                fig_neg = px.bar(
                    neg_df_plot,
                    x='Frekuensi', y='Kata', orientation='h',
                    title=f"Top {len(neg_df_plot)} Kata Negatif",
                    color='Frekuensi', color_continuous_scale='Reds'
                )
                fig_neg.update_layout(height=500, showlegend=False)
                st.plotly_chart(fig_neg, use_container_width=True)
                with st.expander("üìã Tabel Kata Negatif", expanded=show_tables_expanded):
                    st.dataframe(neg_df, use_container_width=True, hide_index=True)

    # =============================
    # 3. ANALISIS N-GRAM
    # =============================
    st.markdown("---")
    st.markdown("### üîÑ Analisis N-Gram")
    st.caption("N-Gram membantu menemukan frasa umum yang muncul bersama dan mengungkap konteks lebih spesifik.")

    with st.expander("‚öôÔ∏è Pengaturan N-Gram", expanded=False):
        col_ng1, col_ng2, col_ng3 = st.columns(3)
        with col_ng1:
            ngram_type = st.radio("Tipe N-Gram:", ["Bigram", "Trigram"], horizontal=True, key="word_analysis_ngram_type")
        with col_ng2:
            top_n_ngrams = st.slider("Top N-Gram:", 5, 30, 10, step=5, key="word_analysis_top_n_ngrams")
        with col_ng3:
            ngram_scope = st.selectbox(
                "Subset Data:", ["Gabungan", "Hanya Positif", "Hanya Negatif"], index=0,
                key="word_analysis_ngram_scope",
                help="Pilih subset data untuk dihitung N-Gram"
            )
    # Defaults if expander collapsed
    if 'ngram_type' not in locals():
        ngram_type = "Bigram"
        top_n_ngrams = 10
        ngram_scope = "Gabungan"

    # Tentukan korpus untuk N-gram
    if 'teks_preprocessing' in df.columns:
        if ngram_scope == "Hanya Positif" and 'predicted_sentiment' in df.columns:
            corpus_text = " ".join(df[df['predicted_sentiment'] == 'POSITIF']['teks_preprocessing'].astype(str))
        elif ngram_scope == "Hanya Negatif" and 'predicted_sentiment' in df.columns:
            corpus_text = " ".join(df[df['predicted_sentiment'] == 'NEGATIF']['teks_preprocessing'].astype(str))
        else:
            corpus_text = " ".join(df['teks_preprocessing'].astype(str))
    else:
        corpus_text = preprocessed_text

    n_val = 2 if ngram_type == "Bigram" else 3
    ngram_data = get_ngrams(corpus_text, n_val, top_n=top_n_ngrams)

    if ngram_data:
        ngram_df = pd.DataFrame(list(ngram_data.items()), columns=['N-Gram', 'Frekuensi'])
        ngram_df_plot = ngram_df.sort_values('Frekuensi', ascending=True)
        fig_ng = px.bar(
            ngram_df_plot,
            x='Frekuensi', y='N-Gram', orientation='h',
            title=f"Top {len(ngram_df_plot)} {ngram_type} Teratas ({ngram_scope})",
            color='Frekuensi', color_continuous_scale='Plasma'
        )
        fig_ng.update_layout(height=600, showlegend=False)
        st.plotly_chart(fig_ng, use_container_width=True)
        with st.expander("üìã Tabel N-Gram"):
            st.dataframe(ngram_df, use_container_width=True, hide_index=True)
    else:
        st.info("üìù Tidak cukup data untuk membentuk N-Gram pada subset yang dipilih.")

    # =============================
    # 4. CATATAN AKHIR
    # =============================
    st.caption("Kata & N-Gram dihitung dari teks hasil preprocessing (token < panjang minimum diabaikan). Word Cloud tersedia pada tab terpisah.")

# ==============================================================================
# FOOTER FUNCTIONS  
# ==============================================================================

def render_footer():
    """Render the application footer."""
    
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; padding: 1rem; background-color: rgba(0,0,0,0.05); border-radius: 0.5rem;">
        <p style="margin: 0; font-size: 0.9rem; color: #666;">
            ¬© 2025 GoRide Sentiment Analysis Dashboard ‚Ä¢ Developed by Mhd Adreansyah
        </p>
        <p style="margin: 0.5rem 0 0 0; font-size: 0.8rem; color: #888;">
            üéì Aplikasi ini merupakan bagian dari Tugas Akhir/Skripsi di bawah perlindungan Hak Cipta
        </p>
    </div>
    """, unsafe_allow_html=True)


# ==============================================================================
# MAIN RENDER FUNCTION
# ==============================================================================

def render_data_analysis() -> None:
    """
    Main function to render the data analysis page.
    
    This function handles the complete workflow:
    1. Authentication and session state initialization
    2. Model loading and validation
    3. File upload and preprocessing configuration
    4. Sentiment analysis processing
    5. Results visualization and analysis tabs
    """
    # Authentication check
    auth.sync_login_state()
    
    # Initialize session state
    initialize_session_state()
    
    # Page header (gunakan key agar tidak reset saat tab interaksi)
    st.title("üìë Analisis Data Teks GoRide")
    st.markdown("---")

    # Ambil model & metrik dari cache
    try:
        data, default_preproc_cached, pipeline, metrics = _cached_model_and_metrics()
        accuracy, precision, recall, f1 = metrics
        if data.empty:
            st.error("‚ùå Data training tidak tersedia untuk analisis!")
            st.stop()
        with st.sidebar:
            st.info(
                f"""ü§ñ **Model Siap Digunakan**\n\nüìä **Performa Model:**\n- Akurasi: {accuracy:.2%}\n- Precision: {precision:.2%}\n- Recall: {recall:.2%}\n- F1-Score: {f1:.2%}\n\nüìà **Data Training:** {len(data):,} ulasan"""
            )
    except Exception as e:
        st.error(f"‚ùå Gagal memuat model (cache): {str(e)}")
        st.stop()
    
    # File upload section
    st.write("### üìÅ Upload File CSV")
    st.info("""
    üìã **Format File yang Didukung:**
    - File CSV dengan kolom teks ulasan
    - Encoding UTF-8 direkomendasikan
    - Kolom 'review_text' akan diprioritaskan, atau Anda dapat memilih kolom lain
    """)
    
    uploaded_file = st.file_uploader(
        "Pilih file CSV untuk dianalisis:",
        type=["csv"],
        key="csv_uploader",
        help="Upload file CSV yang berisi data teks untuk dianalisis sentimen"
    )
    
    # Preprocessing options
    preprocess_options = create_preprocessing_options_ui()
    
    # Analysis button
    analyze_button = st.button(
        "üîç Mulai Analisis Sentimen",
        type="primary",
        disabled=uploaded_file is None
    )
    
    # Handle file processing with better state management
    if uploaded_file is not None and analyze_button:
        # Validate that we have all required components
        if not uploaded_file:
            st.error("‚ö†Ô∏è File tidak tersedia untuk diproses.")
            return
            
        st.session_state.analysis_complete = True
        st.session_state.preprocess_options = preprocess_options
        
        # Clear any selected column from previous runs to avoid conflicts
        if 'selected_text_column' in st.session_state:
            del st.session_state.selected_text_column
        
        # Clear previous results to avoid displaying stale data
        if 'csv_results' in st.session_state:
            del st.session_state.csv_results
        
        # Process file with comprehensive error handling
        with st.spinner("üîÑ Memproses file... Mohon tunggu..."):
            success, processed_df, message = process_uploaded_file(
                uploaded_file, preprocess_options, pipeline
            )
        
        if success and processed_df is not None and not processed_df.empty:
            st.session_state.csv_results = processed_df
            st.success(message)
            
            # Show immediate preview of results
            stats = calculate_sentiment_statistics(processed_df)
            st.info(f"""
            üéâ **Analisis Berhasil Diselesaikan!**
            
            üìä **Ringkasan Hasil:**
            - Total ulasan dianalisis: {stats['total_count']:,}
            - Sentimen Positif: {stats['pos_count']:,} ({stats['pos_percentage']:.1f}%)
            - Sentimen Negatif: {stats['neg_count']:,} ({stats['neg_percentage']:.1f}%)
            - Rata-rata Confidence: {stats['avg_confidence']:.1f}%
            """)
        else:
            st.error(message if message else "‚ùå Gagal memproses file.")
            st.session_state.analysis_complete = False
            
            # Clear any partial results
            if 'csv_results' in st.session_state:
                del st.session_state.csv_results
    
    elif analyze_button and uploaded_file is None:
        st.error("‚ö†Ô∏è Silakan upload file CSV terlebih dahulu!")
    
    # Display results if analysis is complete with comprehensive validation
    if (st.session_state.get('analysis_complete', False) and 
        st.session_state.get('csv_results') is not None):
        
        df = st.session_state.csv_results
        
        # Validate dataframe
        if df is None or df.empty:
            st.warning("‚ö†Ô∏è Tidak ada data yang berhasil diproses.")
            
            # Provide action options
            col1, col2, col3 = st.columns([1, 1, 1])
            with col2:
                if st.button("üîÑ Coba Lagi", use_container_width=True):
                    reset_analysis_state()
                    st.rerun()
            return
        
        # Validate required columns
        required_cols = ['predicted_sentiment', 'confidence', 'review_text']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            st.error(f"‚ùå Kolom yang diperlukan tidak tersedia: {', '.join(missing_cols)}")
            
            # Show available columns for debugging
            with st.expander("üîç Debug Info - Kolom yang Tersedia"):
                st.write("Kolom dalam dataframe:", list(df.columns))
            
            if st.button("üîÑ Reset dan Coba Lagi"):
                reset_analysis_state()
                st.rerun()
            return
        
        # Continue with results display
        st.divider()
        
        # Calculate statistics once for all visualizations
        stats = calculate_sentiment_statistics(df)
        
        # Create sentiment metrics with error handling
        try:
            create_sentiment_metrics(df, stats)
        except Exception as e:
            st.error(f"‚ùå Gagal menampilkan metrics: {str(e)}")
        
        # Create visualizations with error handling
        try:
            create_visualization_charts(df, stats)
        except Exception as e:
            st.error(f"‚ùå Gagal membuat visualisasi: {str(e)}")
            
            # Fallback: show basic statistics (stats already calculated)
            st.write("**üìä Statistik Dasar (Mode Fallback):**")
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Total Ulasan", stats['total_count'])
                st.metric("Sentimen Positif", f"{stats['pos_count']} ({stats['pos_percentage']:.1f}%)")
            with col2:
                st.metric("Sentimen Negatif", f"{stats['neg_count']} ({stats['neg_percentage']:.1f}%)")
                st.metric("Rata-rata Confidence", f"{stats['avg_confidence']:.1f}%")
        
        # Gunakan kolom 'teks_preprocessing' yang sudah ada untuk agregasi cepat (hindari preprocessing ulang)
        try:
            if 'teks_preprocessing' in df.columns:
                preprocessed_all_text = " ".join(df['teks_preprocessing'].dropna().astype(str))
            else:
                # fallback: minimal join teks asli (tidak ideal tapi tetap jalan)
                preprocessed_all_text = " ".join(df['review_text'].dropna().astype(str)) if 'review_text' in df.columns else ""
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Gagal menggabungkan teks untuk analisis kata: {e}")
            preprocessed_all_text = ""
        
        # Render analysis tabs with validation
        st.divider()
        st.write("### üîç Analisis Mendalam")
        
        if preprocessed_all_text.strip():
            try:
                render_analysis_tabs(df, preprocessed_all_text)
            except Exception as e:
                st.error(f"‚ùå Gagal menampilkan analisis mendalam: {str(e)}")
                
                # Fallback: show only results table
                st.write("**üìã Tabel Hasil (Mode Fallback):**")
                display_cols = [col for col in DISPLAY_COLUMNS if col in df.columns]
                if display_cols:
                    st.dataframe(df[display_cols], use_container_width=True)
        else:
            st.warning("‚ö†Ô∏è Tidak ada teks yang tersedia untuk analisis mendalam.")
        
        # Reset button with improved styling
        st.divider()
        col1, col2, col3 = st.columns([1, 1, 1])
        with col2:
            if st.button("üîÑ Analisis File Baru", use_container_width=True, type="secondary"):
                reset_analysis_state()
                st.rerun()
    
    # Render footer
    render_footer()


# ==============================================================================
# ENTRY POINT
# ==============================================================================

if __name__ == "__main__":
    render_data_analysis()
